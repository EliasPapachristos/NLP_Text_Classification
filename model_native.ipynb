{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "VOCAB_FILE_PATH = None # where vocabulary is saved, dynamically set in train_and_eval function\n",
    "PADWORD = 'ZYXW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function to download data from Google Cloud Storage\n",
    "  # Arguments:\n",
    "      source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "      destination: string, the filename to save as on local disk. MUST be filename\n",
    "      ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "  # Returns: nothing, downloads file to local disk\n",
    "\"\"\"\n",
    "def download_from_gcs(source, destination):\n",
    "    search = re.search('gs://(.*?)/(.*)', source)\n",
    "    bucket_name = search.group(1)\n",
    "    blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(blob_name).download_to_filename(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "\n",
    "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.Dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    # Convert texts from python strings to tensors\n",
    "    x = tf.constant(texts)\n",
    "\n",
    "    # Map text to sequence of word-integers and pad\n",
    "    x = vectorize_sentences(x)\n",
    "\n",
    "    # Create tf.data.Dataset from tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "\n",
    "    # Pad to constant length\n",
    "    dataset = dataset.map(pad)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None #loop indefinitley\n",
    "        dataset = dataset.shuffle(buffer_size=50000) # our input is already shuffled so this is redundant\n",
    "    else:\n",
    "        num_epochs = 1\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given an int tensor, remove 0s then pad to a fixed length representation. \n",
    "  #Arguments:\n",
    "    feature: int tensor \n",
    "    label: int. not used in function, just passed through\n",
    "  #Returns:\n",
    "    (int tensor, int) tuple.\n",
    "\"\"\"\n",
    "def pad(feature, label):\n",
    "    # 1. Remove 0s which represent out of vocabulary words\n",
    "    nonzero_indices = tf.where(tf.not_equal(feature, tf.zeros_like(feature)))\n",
    "    without_zeros = tf.gather(feature,nonzero_indices)\n",
    "    without_zeros = tf.squeeze(without_zeros, axis=1)\n",
    "\n",
    "    # 2. Prepend 0s till MAX_SEQUENCE_LENGTH\n",
    "    padded = tf.pad(without_zeros, [[MAX_SEQUENCE_LENGTH, 0]])  # pad out with zeros\n",
    "    padded = padded[-MAX_SEQUENCE_LENGTH:]  # slice to constant length\n",
    "    return (padded, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given sentences, return an integer representation\n",
    "  # Arguments:\n",
    "      sentences: string tensor of shape (?,), contains sentences to vectorize\n",
    "  # Returns:\n",
    "      Integer representation of the sentence. Word-integer mapping is determined\n",
    "        by VOCAB_FILE_PATH. Words out of vocabulary will map to 0\n",
    "\"\"\"\n",
    "def vectorize_sentences(sentences):\n",
    "    # 1. Remove punctuation\n",
    "    sentences = tf.regex_replace(sentences, '[[:punct:]]', ' ')\n",
    "\n",
    "    # 2. Split string tensor into component words\n",
    "    words = tf.string_split(sentences)\n",
    "    words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "\n",
    "    # 3. Map each word to respective integer\n",
    "    table = tf.contrib.lookup.index_table_from_file(\n",
    "        vocabulary_file=VOCAB_FILE_PATH,\n",
    "        num_oov_buckets=0,\n",
    "        vocab_size=None,\n",
    "        default_value=0,  # for words not in vocabulary (OOV)\n",
    "        key_column_index=0,\n",
    "        value_column_index=1,\n",
    "        delimiter=',')\n",
    "    numbers = table.lookup(words)\n",
    "\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_estimator(model_dir,\n",
    "                    config,\n",
    "                    learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    embedding_dim=200,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3,\n",
    "                    embedding_path=None,\n",
    "                    word_index=None):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if embedding_path != None:\n",
    "        embedding_matrix = get_embedding_matrix(word_index, embedding_path, embedding_dim)\n",
    "        is_embedding_trainable = True  # set to False to freeze embedding weights\n",
    "\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    estimator = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir=model_dir, config=config)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines the features to be passed to the model during inference\n",
    "  Can pass in string text directly. Tokenization done in serving_input_fn \n",
    "  # Arguments: none\n",
    "  # Returns: tf.estimator.export.ServingInputReceiver\n",
    "\"\"\"\n",
    "def serving_input_fn():\n",
    "    feature_placeholder = tf.placeholder(tf.string, [None])\n",
    "    features = vectorize_sentences(feature_placeholder)\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes embedding for generic vocabulary and extracts the embeddings\n",
    "  matching the current vocabulary\n",
    "  The pre-trained embedding file is obtained from https://nlp.stanford.edu/projects/glove/\n",
    "  # Arguments: \n",
    "      word_index: dict, {key =word in vocabulary: value= integer mapped to that word}\n",
    "      embedding_path: string, location of the pre-trained embedding file on disk\n",
    "      embedding_dim: int, dimension of the embedding space\n",
    "  # Returns: numpy matrix of shape (vocabulary, embedding_dim) that contains the embedded\n",
    "      representation of each word in the vocabulary.\n",
    "\"\"\"\n",
    "def get_embedding_matrix(word_index, embedding_path, embedding_dim):\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # Download if embedding file is in GCS\n",
    "    if embedding_path.startswith('gs://'):\n",
    "        download_from_gcs(embedding_path, destination='embedding.csv')\n",
    "        embedding_path = 'embedding.csv'\n",
    "\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main orchestrator. Responsible for calling all other functions in model.py\n",
    "  # Arguments: \n",
    "      output_dir: string, file path where training files will be written\n",
    "      hparams: dict, command line parameters passed from task.py\n",
    "  # Returns: nothing, kicks off training and evaluation\n",
    "\"\"\"\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    # Load Data\n",
    "    ((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "        hparams['train_data_path'], hparams['eval_data_path'])\n",
    "\n",
    "    # Create vocabulary from training corpus.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Generate vocabulary file from tokenizer object to enable\n",
    "    # creating a native tensorflow lookup table later (used in vectorize_sentences())\n",
    "    tf.gfile.MkDir(output_dir) # directory must exist before we can use tf.gfile.open\n",
    "    global VOCAB_FILE_PATH; VOCAB_FILE_PATH = os.path.join(output_dir,'vocab.txt')\n",
    "    with tf.gfile.Open(VOCAB_FILE_PATH, 'wb') as f:\n",
    "        f.write(\"{},0\\n\".format(PADWORD))  # map padword to 0\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index < TOP_K: # only save mappings for TOP_K words\n",
    "                f.write(\"{},{}\\n\".format(word, index))\n",
    "\n",
    "    # Create estimator\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_steps=500)\n",
    "    estimator = keras_estimator(\n",
    "        model_dir=output_dir,\n",
    "        config=run_config,\n",
    "        learning_rate=hparams['learning_rate'],\n",
    "        embedding_path=hparams['embedding_path'],\n",
    "        word_index=tokenizer.word_index\n",
    "    )\n",
    "\n",
    "    # Create TrainSpec\n",
    "    train_steps = hparams['num_epochs'] * len(train_texts) / hparams['batch_size']\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=train_steps\n",
    "    )\n",
    "\n",
    "    # Create EvalSpec\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            test_texts,\n",
    "            test_labels,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        start_delay_secs=10,\n",
    "        throttle_secs=10\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
